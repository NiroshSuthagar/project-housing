{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "db49ed31",
   "metadata": {},
   "source": [
    "# Baseline Naive Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 221,
   "id": "9151ef20",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import os \n",
    "from datetime import datetime\n",
    "\n",
    "from dotenv import load_dotenv\n",
    "from openai import OpenAI\n",
    "from pathlib import Path\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score, mean_absolute_error, mean_absolute_percentage_error\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from pathlib import Path\n",
    "import time\n",
    "\n",
    "import duckdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "cbb3c2a7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Name: openai\n",
      "Version: 2.14.0\n",
      "Summary: The official Python library for the openai API\n",
      "Home-page: https://github.com/openai/openai-python\n",
      "Author: \n",
      "Author-email: OpenAI <support@openai.com>\n",
      "License: Apache-2.0\n",
      "Location: /opt/anaconda3/lib/python3.12/site-packages\n",
      "Requires: anyio, distro, httpx, jiter, pydantic, sniffio, tqdm, typing-extensions\n",
      "Required-by: \n"
     ]
    }
   ],
   "source": [
    "!pip show openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6fa23d91",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: openai in /opt/anaconda3/lib/python3.12/site-packages (2.14.0)\n",
      "Requirement already satisfied: anyio<5,>=3.5.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.11.0)\n",
      "Requirement already satisfied: distro<2,>=1.7.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.9.0)\n",
      "Requirement already satisfied: httpx<1,>=0.23.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.28.1)\n",
      "Requirement already satisfied: jiter<1,>=0.10.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (0.11.1)\n",
      "Requirement already satisfied: pydantic<3,>=1.9.0 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (2.11.10)\n",
      "Requirement already satisfied: sniffio in /opt/anaconda3/lib/python3.12/site-packages (from openai) (1.3.0)\n",
      "Requirement already satisfied: tqdm>4 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.66.5)\n",
      "Requirement already satisfied: typing-extensions<5,>=4.11 in /opt/anaconda3/lib/python3.12/site-packages (from openai) (4.15.0)\n",
      "Requirement already satisfied: idna>=2.8 in /opt/anaconda3/lib/python3.12/site-packages (from anyio<5,>=3.5.0->openai) (3.7)\n",
      "Requirement already satisfied: certifi in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (2025.4.26)\n",
      "Requirement already satisfied: httpcore==1.* in /opt/anaconda3/lib/python3.12/site-packages (from httpx<1,>=0.23.0->openai) (1.0.2)\n",
      "Requirement already satisfied: h11<0.15,>=0.13 in /opt/anaconda3/lib/python3.12/site-packages (from httpcore==1.*->httpx<1,>=0.23.0->openai) (0.14.0)\n",
      "Requirement already satisfied: annotated-types>=0.6.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.6.0)\n",
      "Requirement already satisfied: pydantic-core==2.33.2 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (2.33.2)\n",
      "Requirement already satisfied: typing-inspection>=0.4.0 in /opt/anaconda3/lib/python3.12/site-packages (from pydantic<3,>=1.9.0->openai) (0.4.2)\n"
     ]
    }
   ],
   "source": [
    "!pip install --upgrade openai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "55935c68",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loading the stratified features \n",
    "X_train = pd.read_parquet('data_output/prod_features/X_train.parquet')\n",
    "y_train = pd.read_parquet('data_output/prod_features/y_train.parquet')\n",
    "X_test = pd.read_parquet('data_output/prod_features/X_test.parquet')\n",
    "y_test = pd.read_parquet('data_output/prod_features/y_test.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "0cbee1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Partioning Complete: 87 Partitions Created\n",
      "Partioning Complete: 87 Partitions Created\n"
     ]
    }
   ],
   "source": [
    "# Partitioning data into chunks of 100 records (to monitor cost efficiency)\n",
    "output_dir = 'data_output/prod_features/json_test_files/'\n",
    "partition_size = 100\n",
    "\n",
    "X_test = X_test.reset_index(drop=True)\n",
    "y_test = y_test.reset_index(drop=True)\n",
    "\n",
    "df_to_chunk ={'X_test':X_test, 'y_test':y_test}\n",
    "\n",
    "for name, file in df_to_chunk.items():\n",
    "\n",
    "    for chunk_id, chunk_df in iter(file.groupby(np.arange(len(file)) // partition_size)):\n",
    "        file_path = f'{output_dir}{name}_part_{chunk_id}.jsonl'\n",
    "        chunk_df.to_json(file_path,orient='records', lines=True)\n",
    "\n",
    "    print(f'Partioning Complete: {chunk_id} Partitions Created')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "id": "c3dccbb7",
   "metadata": {},
   "outputs": [],
   "source": [
    "class NaiveAgent:\n",
    "    def __init__(self, system_prompt = None, model = 'gpt-5-nano' ):\n",
    "        load_dotenv()\n",
    "        \n",
    "        api_key = os.getenv('OPENAI_API_KEY')\n",
    "        if not api_key:\n",
    "            raise ValueError('OpenAI API Key not found in .env')\n",
    "        \n",
    "        self.client = OpenAI(api_key=api_key)\n",
    "        \n",
    "        self.model = model\n",
    "        self.system_prompt = system_prompt\n",
    "        self.user_prompt = None\n",
    "        self.base_data_path = Path('data_output/prod_features/json_test_files/')\n",
    "        self.output_dir = Path('data_output/agentic_outputs')\n",
    "        self.input_data = None\n",
    "    \n",
    "    def JSON_read(self, file_name, output_dir='data_output'):\n",
    "        try:\n",
    "            data = []\n",
    "            self.file_path = self.base_data_path / file_name\n",
    "            with self.file_path.open('r', encoding='utf-8') as f:\n",
    "                for line in f:\n",
    "                    data.append(json.loads(line.strip()))\n",
    "            return data\n",
    "        except Exception as e:\n",
    "            print(f'Error Loading JSON: \\n{e}')\n",
    "            return None\n",
    "    \n",
    "    def call_agent(self):   \n",
    "        str_user_prompt = str(self.user_prompt)\n",
    "        str_sys_prompt = str(self.system_prompt)\n",
    "        \n",
    "        agent_response = self.client.responses.create(\n",
    "            model = self.model,\n",
    "            input = [\n",
    "                {\n",
    "                    \"role\":\"system\",\n",
    "                    \"content\":str_sys_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\":str_user_prompt\n",
    "                }\n",
    "            ],\n",
    "            text = {\n",
    "                \"format\":{\n",
    "                    \"type\":\"json_schema\",\n",
    "                    \"name\":\"agent_output\",\n",
    "                    \"strict\":True,\n",
    "                    \"schema\": {\n",
    "                        \"type\":\"object\",\n",
    "                        \"properties\":{\n",
    "                                        \"price\":{\"type\":\"number\"}\n",
    "                                    },\n",
    "                                    \"required\":[\"price\"],\n",
    "                                    \"additionalProperties\": False\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "    )\n",
    "        output = json.loads(agent_response.output_text)\n",
    "        return output\n",
    "    \n",
    "    def batch_predict(self, file_name, output_dir='data_output/agentic_preds/', output_file='preds.jsonl'):\n",
    "        data = self.JSON_read(file_name)\n",
    "        if not data:\n",
    "            return\n",
    "        results = []\n",
    "        \n",
    "        for i, record in enumerate(tqdm(data, desc=f\"Processing Test Data Predictions ({file_name})\")):\n",
    "            self.user_prompt = f'Predict the price for this item:\\n{json.dumps(record,indent=2)}'\n",
    "            prediction = self.call_agent()\n",
    "        \n",
    "            if prediction:\n",
    "                result = {\n",
    "                    'predicted_price': prediction['price'],\n",
    "                    'pred_id':i\n",
    "                }\n",
    "                results.append(result)\n",
    "        \n",
    "        # Saving results as JSONL\n",
    "        \n",
    "        output_dir = Path(output_dir) \n",
    "        start = Path(file_name).stem #before the .jsonl\n",
    "        end = Path(file_name).suffix\n",
    "        output_file_name = f'{start}_preds{end}'\n",
    "        output_path = output_dir / output_file_name\n",
    "        \n",
    "        with output_path.open('w', encoding='utf-8') as f:\n",
    "            for result in results:\n",
    "                f.write(json.dumps(result) + '\\n')\n",
    "        \n",
    "        print(f'Saved {len(results)} predictions at {str(output_path)}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77cc9659",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions: 100%|██████████| 100/100 [17:38<00:00, 10.58s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_0_preds.jsonl\n",
      "Predictions were completed in 1058.4592680931091\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "start_time = time.time()\n",
    "\n",
    "agent_naive = NaiveAgent(\n",
    "    system_prompt=(\n",
    "    'Predict the house price in GBP based on what you know and the information provided.'\n",
    "    )\n",
    "    )\n",
    "chunk1_response = agent_naive.batch_predict(file_name='X_test_part_0.jsonl')\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f'Predictions were completed in {duration}')\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f6481b07",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_0.jsonl): 100%|██████████| 100/100 [17:54<00:00, 10.74s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_0_preds.jsonl\n",
      "Predictions for X_test_part_0.jsonl were completed in 1074.3598289489746\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_0.jsonl): 100%|██████████| 100/100 [15:06<00:00,  9.06s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v2/X_test_part_0_preds.jsonl\n",
      "Predictions for X_test_part_0.jsonl were completed in 1980.4088780879974\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_0.jsonl): 100%|██████████| 100/100 [22:35<00:00, 13.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v3/X_test_part_0_preds.jsonl\n",
      "Predictions for X_test_part_0.jsonl were completed in 3335.9625358581543\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_1.jsonl): 100%|██████████| 100/100 [19:06<00:00, 11.46s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_1_preds.jsonl\n",
      "Predictions for X_test_part_1.jsonl were completed in 4482.0299689769745\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_1.jsonl): 100%|██████████| 100/100 [21:59<00:00, 13.20s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v2/X_test_part_1_preds.jsonl\n",
      "Predictions for X_test_part_1.jsonl were completed in 5801.821666955948\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_1.jsonl): 100%|██████████| 100/100 [25:15<00:00, 15.15s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v3/X_test_part_1_preds.jsonl\n",
      "Predictions for X_test_part_1.jsonl were completed in 7317.165796041489\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_2.jsonl): 100%|██████████| 100/100 [15:22<00:00,  9.22s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_2_preds.jsonl\n",
      "Predictions for X_test_part_2.jsonl were completed in 8239.650177001953\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_2.jsonl): 100%|██████████| 100/100 [18:17<00:00, 10.98s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v2/X_test_part_2_preds.jsonl\n",
      "Predictions for X_test_part_2.jsonl were completed in 9337.50428199768\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_2.jsonl): 100%|██████████| 100/100 [22:15<00:00, 13.36s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v3/X_test_part_2_preds.jsonl\n",
      "Predictions for X_test_part_2.jsonl were completed in 10673.335622787476\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_3.jsonl): 100%|██████████| 100/100 [18:38<00:00, 11.19s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_3_preds.jsonl\n",
      "Predictions for X_test_part_3.jsonl were completed in 11791.956265926361\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_3.jsonl): 100%|██████████| 100/100 [22:33<00:00, 13.54s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v2/X_test_part_3_preds.jsonl\n",
      "Predictions for X_test_part_3.jsonl were completed in 13145.724818944931\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_3.jsonl): 100%|██████████| 100/100 [22:36<00:00, 13.57s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v3/X_test_part_3_preds.jsonl\n",
      "Predictions for X_test_part_3.jsonl were completed in 14502.584130048752\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_4.jsonl): 100%|██████████| 100/100 [16:37<00:00,  9.97s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_4_preds.jsonl\n",
      "Predictions for X_test_part_4.jsonl were completed in 15499.85547709465\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_4.jsonl): 100%|██████████| 100/100 [20:47<00:00, 12.48s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v2/X_test_part_4_preds.jsonl\n",
      "Predictions for X_test_part_4.jsonl were completed in 16747.66389298439\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_4.jsonl): 100%|██████████| 100/100 [22:16<00:00, 13.37s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v3/X_test_part_4_preds.jsonl\n",
      "Predictions for X_test_part_4.jsonl were completed in 18084.620153188705\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_5.jsonl): 100%|██████████| 100/100 [16:42<00:00, 10.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_5_preds.jsonl\n",
      "Predictions for X_test_part_5.jsonl were completed in 19087.33723282814\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_5.jsonl): 100%|██████████| 100/100 [16:43<00:00, 10.03s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v2/X_test_part_5_preds.jsonl\n",
      "Predictions for X_test_part_5.jsonl were completed in 20090.384097099304\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_5.jsonl): 100%|██████████| 100/100 [20:54<00:00, 12.55s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v3/X_test_part_5_preds.jsonl\n",
      "Predictions for X_test_part_5.jsonl were completed in 21345.41723704338\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_6.jsonl): 100%|██████████| 100/100 [15:08<00:00,  9.08s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_6_preds.jsonl\n",
      "Predictions for X_test_part_6.jsonl were completed in 22253.703226089478\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_6.jsonl): 100%|██████████| 100/100 [21:45<00:00, 13.05s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v2/X_test_part_6_preds.jsonl\n",
      "Predictions for X_test_part_6.jsonl were completed in 23559.203674077988\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_6.jsonl): 100%|██████████| 100/100 [20:30<00:00, 12.30s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v3/X_test_part_6_preds.jsonl\n",
      "Predictions for X_test_part_6.jsonl were completed in 24789.369733810425\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_7.jsonl): 100%|██████████| 100/100 [15:51<00:00,  9.51s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_7_preds.jsonl\n",
      "Predictions for X_test_part_7.jsonl were completed in 25740.455826997757\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_7.jsonl): 100%|██████████| 100/100 [14:42<00:00,  8.83s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v2/X_test_part_7_preds.jsonl\n",
      "Predictions for X_test_part_7.jsonl were completed in 26623.481128931046\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_7.jsonl): 100%|██████████| 100/100 [17:41<00:00, 10.61s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v3/X_test_part_7_preds.jsonl\n",
      "Predictions for X_test_part_7.jsonl were completed in 27684.674728870392\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_8.jsonl): 100%|██████████| 100/100 [21:30<00:00, 12.91s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds/X_test_part_8_preds.jsonl\n",
      "Predictions for X_test_part_8.jsonl were completed in 28975.562529087067\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_8.jsonl): 100%|██████████| 100/100 [17:36<00:00, 10.56s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v2/X_test_part_8_preds.jsonl\n",
      "Predictions for X_test_part_8.jsonl were completed in 30031.826627016068\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_8.jsonl): 100%|██████████| 100/100 [20:28<00:00, 12.28s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/agentic_preds_v3/X_test_part_8_preds.jsonl\n",
      "Predictions for X_test_part_8.jsonl were completed in 31259.928653001785\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "# Experiements with various system prompts.\n",
    "\n",
    "start_time = time.time()\n",
    "\n",
    "prompts_list = [\n",
    "    ''' \n",
    "    Predict the house price in GBP based on what you know and the information provided.\n",
    "    ''',\n",
    "    '''\n",
    "    You are a real estate market analyst with expertise in house price valuation. \n",
    "    Predict house prices in GBP using the EPC data provided while accounting for local market context and domain specific knowledge that would influence the price.\n",
    "    ''',\n",
    "    ''' \n",
    "    You are a real estate market analyst with expertise in house price valuation. \n",
    "    Predict house prices using the EPC data provided while accounting for local market context and domain specific knowledge that would influence the price.\n",
    "    \n",
    "    Consider the following framework for your analysis and make your prediction for the house price in GBP:\n",
    "    - Area Analysis\n",
    "        - What is the property value like on average for different types of properties. \n",
    "        - What are some price drivers like schools and transport?\n",
    "    - Feature selection\n",
    "        - What are EPC features that signal premium or undesirable price factors? For example underfloor heating is a positive factor. \n",
    "    - Temporal Effect\n",
    "        - Consider how EPC ratings have changed over time in this market.\n",
    "        - Are there any patterns or socio-economic factors that could influece the price prediction. \n",
    "        - Are there any policy changes to consider that could impact house prices?\n",
    "    \n",
    "    Considering the above, provide a house price prediction that is likely to be accurate.\n",
    "    '''\n",
    "]\n",
    "\n",
    "output_dir = ['data_output/agentic_preds', 'data_output/agentic_preds_v2', 'data_output/agentic_preds_v3']\n",
    "files_considered  = ['X_test_part_0.jsonl', \n",
    "                     'X_test_part_1.jsonl', \n",
    "                     'X_test_part_2.jsonl', \n",
    "                     'X_test_part_3.jsonl',\n",
    "                     'X_test_part_4.jsonl',\n",
    "                     'X_test_part_5.jsonl',\n",
    "                     'X_test_part_6.jsonl',\n",
    "                     'X_test_part_7.jsonl',\n",
    "                     'X_test_part_8.jsonl']\n",
    "\n",
    "for file in files_considered:\n",
    "    for prompt, dir in zip(prompts_list, output_dir):  \n",
    "        try: \n",
    "            agent_naive = NaiveAgent(\n",
    "                system_prompt=prompt\n",
    "                )\n",
    "            chunk1_response = agent_naive.batch_predict(file_name=file, output_dir=dir) \n",
    "\n",
    "            end_time = time.time()\n",
    "            duration = end_time - start_time\n",
    "            print(f'Predictions for {file} were completed in {duration}')\n",
    "        except:\n",
    "            print('***')\n",
    "            print(f'Something went wrong for the file {file}')\n",
    "            print('***')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c7b59239",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 311629.8876407717 \n",
      "Test MAE: 177110.9 \n",
      "Test MAPE: 26.2%\n"
     ]
    }
   ],
   "source": [
    "# Evaluating the Naive Model V1 Predictions\n",
    "y_preds_fp = 'data_output/agentic_preds/X_test_part_0_preds.jsonl'\n",
    "y_actuals_fp = 'data_output/prod_features/json_test_files/y_test_part_0.jsonl'\n",
    "\n",
    "y_preds_AI = pd.read_json(y_preds_fp, lines=True)\n",
    "y_preds_AI = y_preds_AI['predicted_price'] # I only need the predicted price column.\n",
    "y_actuals= pd.read_json(y_actuals_fp, lines=True, chunksize=None)\n",
    "\n",
    "test_rmse_AI = np.sqrt(mean_squared_error(y_actuals, y_preds_AI))\n",
    "test_mae_AI = mean_absolute_error(y_actuals,y_preds_AI)\n",
    "test_mape_AI = round(mean_absolute_percentage_error(y_actuals,y_preds_AI)*100,2)\n",
    "\n",
    "\n",
    "print(f'Test RMSE: {test_rmse_AI} \\nTest MAE: {test_mae_AI} \\nTest MAPE: {test_mape_AI}%')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "id": "1e350c5a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_test_part_0_preds.jsonl', 'X_test_part_1_preds.jsonl', 'X_test_part_2_preds.jsonl', 'X_test_part_3_preds.jsonl', 'X_test_part_4_preds.jsonl', 'X_test_part_5_preds.jsonl', 'X_test_part_6_preds.jsonl', 'X_test_part_7_preds.jsonl', 'X_test_part_8_preds.jsonl']\n",
      "['y_test_part_0.jsonl', 'y_test_part_1.jsonl', 'y_test_part_2.jsonl', 'y_test_part_3.jsonl', 'y_test_part_4.jsonl', 'y_test_part_5.jsonl', 'y_test_part_6.jsonl', 'y_test_part_7.jsonl', 'y_test_part_8.jsonl']\n"
     ]
    }
   ],
   "source": [
    "# Iterating over all available the chunks for the predicitons\n",
    "number_of_partitions = 9 \n",
    "y_actuals_list = []\n",
    "y_preds_list = []\n",
    "\n",
    "for n in range(number_of_partitions): \n",
    "    y_actuals_file = f'y_test_part_{str(n)}.jsonl'\n",
    "    y_preds_file = f'X_test_part_{str(n)}_preds.jsonl'\n",
    "    \n",
    "    y_actuals_list.append(y_actuals_file)\n",
    "    y_preds_list.append(y_preds_file)\n",
    "\n",
    "print(y_preds_list)\n",
    "print(y_actuals_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2883593c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test RMSE: 311629.8876407717 \n",
      "Test MAE: 177110.9 \n",
      "Test MAPE: 26.2%\n"
     ]
    }
   ],
   "source": [
    "folders = ['agentic_preds', 'agentic_preds_v2', 'agentic_preds_v3']\n",
    "\n",
    "# Evaluating all available chunks:\n",
    "rmse_AI = []\n",
    "mae_AI = []\n",
    "mape_AI = []\n",
    "\n",
    "for actual,pred in zip(y_actuals_list, y_preds_list):\n",
    "    y_preds_fp = Path('data_output/agentic_preds/') / pred\n",
    "    y_actuals_fp = Path('data_output/prod_features/json_test_files/') / actual\n",
    "\n",
    "    y_preds_AI = pd.read_json(y_preds_fp, lines=True)\n",
    "    y_preds_AI = y_preds_AI['predicted_price'] # I only need the predicted price column.\n",
    "    y_actuals= pd.read_json(y_actuals_fp, lines=True, chunksize=None)\n",
    "\n",
    "    test_rmse_AI = np.sqrt(mean_squared_error(y_actuals, y_preds_AI))\n",
    "    rmse_AI.append(test_rmse_AI)\n",
    "    test_mae_AI = mean_absolute_error(y_actuals,y_preds_AI)\n",
    "    mae_AI.append(test_mae_AI)\n",
    "    test_mape_AI = round(mean_absolute_percentage_error(y_actuals,y_preds_AI)*100,2) \n",
    "    mape_AI.append(test_mape_AI)\n",
    "\n",
    "print(f'Test RMSE: {test_rmse_AI} \\nTest MAE: {test_mae_AI} \\nTest MAPE: {test_mape_AI}%')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe421b86",
   "metadata": {},
   "source": [
    "## 2. Multi-Agent Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "id": "d5e29ea0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>1HE_district</th>\n",
       "      <th>1HE_CURRENT_ENERGY_RATING</th>\n",
       "      <th>1HE_POTENTIAL_ENERGY_RATING</th>\n",
       "      <th>1HE_PROPERTY_TYPE</th>\n",
       "      <th>1HE_BUILT_FORM</th>\n",
       "      <th>1HE_ENERGY_TARIFF</th>\n",
       "      <th>1HE_MAINS_GAS_FLAG</th>\n",
       "      <th>1HE_GLAZED_AREA</th>\n",
       "      <th>1HE_HOT_WATER_ENERGY_EFF</th>\n",
       "      <th>1HE_HOT_WATER_ENV_EFF</th>\n",
       "      <th>...</th>\n",
       "      <th>CORE_HEATING_COST_POTENTIAL</th>\n",
       "      <th>CORE_HOT_WATER_COST_CURRENT</th>\n",
       "      <th>CORE_HOT_WATER_COST_POTENTIAL</th>\n",
       "      <th>CORE_TOTAL_FLOOR_AREA</th>\n",
       "      <th>CORE_MULTI_GLAZE_PROPORTION</th>\n",
       "      <th>CORE_EXTENSION_COUNT</th>\n",
       "      <th>CORE_NUMBER_HABITABLE_ROOMS</th>\n",
       "      <th>CORE_NUMBER_HEATED_ROOMS</th>\n",
       "      <th>CORE_LOW_ENERGY_LIGHTING</th>\n",
       "      <th>CORE_NUMBER_OPEN_FIREPLACES</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BUCKINGHAMSHIRE</td>\n",
       "      <td>D</td>\n",
       "      <td>C</td>\n",
       "      <td>House</td>\n",
       "      <td>End-Terrace</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>Not Available</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>327.0</td>\n",
       "      <td>118.0</td>\n",
       "      <td>103.0</td>\n",
       "      <td>88.240</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>BUCKINGHAMSHIRE</td>\n",
       "      <td>C</td>\n",
       "      <td>C</td>\n",
       "      <td>House</td>\n",
       "      <td>Mid-Terrace</td>\n",
       "      <td>Single</td>\n",
       "      <td>Y</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>282.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>74.0</td>\n",
       "      <td>48.794</td>\n",
       "      <td>100.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>SLOUGH</td>\n",
       "      <td>E</td>\n",
       "      <td>C</td>\n",
       "      <td>House</td>\n",
       "      <td>Semi-Detached</td>\n",
       "      <td>Single</td>\n",
       "      <td>Y</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>511.0</td>\n",
       "      <td>85.0</td>\n",
       "      <td>54.0</td>\n",
       "      <td>73.000</td>\n",
       "      <td>13.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>5.0</td>\n",
       "      <td>83.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>BUCKINGHAMSHIRE</td>\n",
       "      <td>D</td>\n",
       "      <td>D</td>\n",
       "      <td>House</td>\n",
       "      <td>Mid-Terrace</td>\n",
       "      <td>Single</td>\n",
       "      <td>Y</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Good</td>\n",
       "      <td>Good</td>\n",
       "      <td>...</td>\n",
       "      <td>589.0</td>\n",
       "      <td>93.0</td>\n",
       "      <td>82.0</td>\n",
       "      <td>72.700</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>78.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>WINDSOR AND MAIDENHEAD</td>\n",
       "      <td>F</td>\n",
       "      <td>C</td>\n",
       "      <td>House</td>\n",
       "      <td>Mid-Terrace</td>\n",
       "      <td>Single</td>\n",
       "      <td>Y</td>\n",
       "      <td>Normal</td>\n",
       "      <td>Average</td>\n",
       "      <td>Average</td>\n",
       "      <td>...</td>\n",
       "      <td>648.0</td>\n",
       "      <td>102.0</td>\n",
       "      <td>81.0</td>\n",
       "      <td>84.000</td>\n",
       "      <td>5.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>4.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 42 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "             1HE_district 1HE_CURRENT_ENERGY_RATING  \\\n",
       "0         BUCKINGHAMSHIRE                         D   \n",
       "1         BUCKINGHAMSHIRE                         C   \n",
       "2                  SLOUGH                         E   \n",
       "3         BUCKINGHAMSHIRE                         D   \n",
       "4  WINDSOR AND MAIDENHEAD                         F   \n",
       "\n",
       "  1HE_POTENTIAL_ENERGY_RATING 1HE_PROPERTY_TYPE 1HE_BUILT_FORM  \\\n",
       "0                           C             House    End-Terrace   \n",
       "1                           C             House    Mid-Terrace   \n",
       "2                           C             House  Semi-Detached   \n",
       "3                           D             House    Mid-Terrace   \n",
       "4                           C             House    Mid-Terrace   \n",
       "\n",
       "  1HE_ENERGY_TARIFF 1HE_MAINS_GAS_FLAG 1HE_GLAZED_AREA  \\\n",
       "0           Unknown      Not Available          Normal   \n",
       "1            Single                  Y          Normal   \n",
       "2            Single                  Y          Normal   \n",
       "3            Single                  Y          Normal   \n",
       "4            Single                  Y          Normal   \n",
       "\n",
       "  1HE_HOT_WATER_ENERGY_EFF 1HE_HOT_WATER_ENV_EFF  ...  \\\n",
       "0                     Good                  Good  ...   \n",
       "1                     Good                  Good  ...   \n",
       "2                     Good                  Good  ...   \n",
       "3                     Good                  Good  ...   \n",
       "4                  Average               Average  ...   \n",
       "\n",
       "  CORE_HEATING_COST_POTENTIAL CORE_HOT_WATER_COST_CURRENT  \\\n",
       "0                       327.0                       118.0   \n",
       "1                       282.0                        85.0   \n",
       "2                       511.0                        85.0   \n",
       "3                       589.0                        93.0   \n",
       "4                       648.0                       102.0   \n",
       "\n",
       "  CORE_HOT_WATER_COST_POTENTIAL CORE_TOTAL_FLOOR_AREA  \\\n",
       "0                         103.0                88.240   \n",
       "1                          74.0                48.794   \n",
       "2                          54.0                73.000   \n",
       "3                          82.0                72.700   \n",
       "4                          81.0                84.000   \n",
       "\n",
       "  CORE_MULTI_GLAZE_PROPORTION CORE_EXTENSION_COUNT  \\\n",
       "0                       100.0                  0.0   \n",
       "1                       100.0                  0.0   \n",
       "2                        13.0                  2.0   \n",
       "3                         0.0                  0.0   \n",
       "4                         5.0                  2.0   \n",
       "\n",
       "  CORE_NUMBER_HABITABLE_ROOMS CORE_NUMBER_HEATED_ROOMS  \\\n",
       "0                         5.0                      5.0   \n",
       "1                         3.0                      3.0   \n",
       "2                         5.0                      5.0   \n",
       "3                         4.0                      4.0   \n",
       "4                         4.0                      2.0   \n",
       "\n",
       "  CORE_LOW_ENERGY_LIGHTING CORE_NUMBER_OPEN_FIREPLACES  \n",
       "0                      0.0                         0.0  \n",
       "1                      0.0                         0.0  \n",
       "2                     83.0                         1.0  \n",
       "3                     78.0                         1.0  \n",
       "4                      0.0                         1.0  \n",
       "\n",
       "[5 rows x 42 columns]"
      ]
     },
     "execution_count": 141,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train\n",
    "\n",
    "results = duckdb.sql('SELECT * FROM X_train LIMIT 10').df()\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "id": "5b07ea79",
   "metadata": {},
   "outputs": [],
   "source": [
    "agent_list = {\n",
    "    \"AnalystAgent\":\n",
    "        \"\"\"\n",
    "        Research average prices for houses given the main features:\n",
    "        1HE_district\n",
    "        1HE_PROPERTY TYPE \n",
    "        1HE_BUILT_FORM \n",
    "        CORE_TOTAL_FLOOR_AREA \n",
    "        CORE_EXTENSION_COUNT \n",
    "        CORE_NUMBER_OF_HABITABLE_ROOMS\n",
    "        \n",
    "        Consider the other features if necessary and provide a short summary for the Prediction agent to predict the house price for this particular record. Limit your summary to 100 words! \n",
    "        \"\"\",\n",
    "    \"PredictionAgent\":\n",
    "        \"\"\"\n",
    "        Using data obtained from the Analyst Agent, make a prediction for the house price in GBP for the JSON input. \n",
    "        Focus on and consider the following core features from the JSON input to make your prediction but you can use the other columns should you deem it necessary:\n",
    "        1HE_district\n",
    "        1HE_PROPERTY TYPE \n",
    "        1HE_BUILT_FORM \n",
    "        CORE_TOTAL_FLOOR_AREA \n",
    "        CORE_EXTENSION_COUNT \n",
    "        CORE_NUMBER_OF_HABITABLE_ROOMS\n",
    "        \"\"\"\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "id": "49afd8b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_data = str({\"1HE_district\":\"SLOUGH\",\"1HE_CURRENT_ENERGY_RATING\":\"D\",\"1HE_POTENTIAL_ENERGY_RATING\":\"B\",\"1HE_PROPERTY_TYPE\":\"House\",\"1HE_BUILT_FORM\":\"Semi-Detached\",\"1HE_ENERGY_TARIFF\":\"Single\",\"1HE_MAINS_GAS_FLAG\":\"Y\",\"1HE_GLAZED_AREA\":\"Normal\",\"1HE_HOT_WATER_ENERGY_EFF\":\"Good\",\"1HE_HOT_WATER_ENV_EFF\":\"Good\",\"1HE_WINDOWS_ENERGY_EFF\":\"Average\",\"1HE_WINDOWS_ENV_EFF\":\"Average\",\"1HE_WALLS_ENERGY_EFF\":\"Average\",\"1HE_WALLS_ENV_EFF\":\"Average\",\"1HE_ROOF_ENERGY_EFF\":\"Poor\",\"1HE_MAINHEAT_ENERGY_EFF\":\"Good\",\"1HE_MAINHEATC_ENERGY_EFF\":\"Good\",\"1HE_LIGHTING_ENERGY_EFF\":\"Very Good\",\"1HE_MECHANICAL_VENTILATION\":\"natural\",\"1HE_TENURE\":\"Owner Occupied\",\"CORE_CURRENT_ENERGY_EFFICIENCY\":67.0,\"CORE_POTENTIAL_ENERGY_EFFICIENCY\":83.0,\"CORE_ENVIRONMENT_IMPACT_CURRENT\":63.0,\"CORE_ENVIRONMENT_IMPACT_POTENTIAL\":80.0,\"CORE_ENERGY_CONSUMPTION_CURRENT\":219.0,\"CORE_ENERGY_CONSUMPTION_POTENTIAL\":105.0,\"CORE_CO2_EMISSIONS_CURRENT\":3.2,\"CORE_CO2_EMISS_CURR_PER_FLOOR_AREA\":39.0,\"CORE_CO2_EMISSIONS_POTENTIAL\":1.6,\"CORE_LIGHTING_COST_CURRENT\":72.0,\"CORE_LIGHTING_COST_POTENTIAL\":72.0,\"CORE_HEATING_COST_CURRENT\":540.0,\"CORE_HEATING_COST_POTENTIAL\":461.0,\"CORE_HOT_WATER_COST_CURRENT\":94.0,\"CORE_HOT_WATER_COST_POTENTIAL\":66.0,\"CORE_TOTAL_FLOOR_AREA\":82.0,\"CORE_MULTI_GLAZE_PROPORTION\":100.0,\"CORE_EXTENSION_COUNT\":0.0,\"CORE_NUMBER_HABITABLE_ROOMS\":5.0,\"CORE_NUMBER_HEATED_ROOMS\":5.0,\"CORE_LOW_ENERGY_LIGHTING\":100.0,\"CORE_NUMBER_OPEN_FIREPLACES\":0.0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "id": "fa6b67ea",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MultiAgent(NaiveAgent):\n",
    "    \n",
    "    def __init__(self, agent_list,max_agents = 5, model='gpt-5-nano'):\n",
    "        super().__init__(model) # Initialising the parent class. \n",
    "        self.memory = []\n",
    "        self.max_agents = max_agents\n",
    "        self.agent_list = agent_list\n",
    "        self.analyst_sys_prompt = self.agent_list['AnalystAgent']\n",
    "        self.predictor_sys_prompt = self.agent_list['PredictionAgent']\n",
    "        self.rag_errors = 0\n",
    "        \n",
    "        #Initialising RAG database in-memory - descroped as SQL prompts were highly erroneous. \n",
    "        #self.conn = duckdb.connect(':memory:')\n",
    "        #self.conn.execute(f\"CREATE TABLE masterRAG AS SELECT * FROM '{'data_output/RAGdb/RAGdb.parquet'}'\")\n",
    "        #load_count = self.conn.execute(\"SELECT COUNT(*) FROM X_train\").fetchone()[0]\n",
    "        #print(f'Loaded {load_count} records into memory.')\n",
    "        \n",
    "        if len(agent_list) > self.max_agents: # Validation to prevent high costs\n",
    "            raise ValueError(f\"You have more than {self.max_agents} agents, please reduce n_agents or increase max_agents.\")\n",
    "        \n",
    "    def initialise_Analyst(self, start_prompt):\n",
    "        self.memory = []\n",
    "        self.memory.append(start_prompt)\n",
    "        \n",
    "        # Formatting the start prompt as a str (JSON file read in)\n",
    "        if isinstance(start_prompt, (dict, pd.Series)):\n",
    "            prompt_text = \"\\n\".join([f'{k}:{v}' for k, v in start_prompt.items()])\n",
    "        else: \n",
    "            prompt_text = str(start_prompt)\n",
    "            \n",
    "        self.analyst_response = self.client.responses.create(\n",
    "        model = self.model,\n",
    "        input = [\n",
    "            {\n",
    "                \"role\":\"system\",\n",
    "                \"content\":self.analyst_sys_prompt\n",
    "            },\n",
    "            {\n",
    "                \"role\":\"user\",\n",
    "                \"content\": prompt_text\n",
    "            }\n",
    "        ]\n",
    "        )\n",
    "        #self.memory.append(self.analyst_response.output_text)\n",
    "        #augmentation = self.rag(sql= self.analyst_response.output_text)\n",
    "        \n",
    "        self.memory.append(\n",
    "                f'Data To Support Predicton:\\n{self.analyst_response.output_text}'\n",
    "            )\n",
    "        \n",
    "        \"\"\" \n",
    "        # Removed due to poor quality SQL scripts from AnalystAgent.\n",
    "        def rag(self, sql):\n",
    "        print(f'Starting RAG with the following SQL Query: \\n{self.analyst_response.output_text}')\n",
    "        try:\n",
    "            results = self.conn.execute(sql).fetch_df()\n",
    "            json_results = results.to_json(orient='records', indent=2)\n",
    "            return json_results\n",
    "            \n",
    "        except:\n",
    "            self.rag_errors += 1\n",
    "            print('Something went wrong generating the SQL RAG Output. RAG Failed for this records.')\n",
    "            return 'No augmentation data available.'\n",
    "        \"\"\"\n",
    "\n",
    "    def initialise_Predictor(self):\n",
    "        \n",
    "        context = '\\n'.join([str(item) for item in self.memory])\n",
    "        self.prediction_context = context\n",
    "        \n",
    "        predictor_prompt = f'Context From Previous Analysis:\\n{context}\\nMake your prediction based on this data.'\n",
    "        \n",
    "        predictor_response = self.client.responses.create(\n",
    "            model = self.model,\n",
    "            input = [\n",
    "                {\n",
    "                    \"role\":\"system\",\n",
    "                    \"content\":self.predictor_sys_prompt\n",
    "                },\n",
    "                {\n",
    "                    \"role\":\"user\",\n",
    "                    \"content\": predictor_prompt\n",
    "                }\n",
    "            ],\n",
    "            text = {\n",
    "                \"format\":{\n",
    "                    \"type\":\"json_schema\",\n",
    "                    \"name\":\"agent_output\",\n",
    "                    \"strict\":True,\n",
    "                    \"schema\": {\n",
    "                        \"type\":\"object\",\n",
    "                        \"properties\":{\n",
    "                                        \"price\":{\"type\":\"number\"}\n",
    "                                    },\n",
    "                                    \"required\":[\"price\"],\n",
    "                                    \"additionalProperties\": False\n",
    "                                }\n",
    "                            }\n",
    "                        }\n",
    "        )\n",
    "        self.batch_output = json.loads(predictor_response.output_text)\n",
    "        self.memory.append(self.batch_output)\n",
    "        #print(f'Prediction: {self.batch_output}') #Used for testing purpose\n",
    "        #print(f'Context used:{predictor_prompt} ') #Used for testing purpose\n",
    "        \n",
    "    def main(self,start_prompt):\n",
    "        self.initialise_Analyst(start_prompt=start_prompt)\n",
    "        self.initialise_Predictor()\n",
    "    \n",
    "    def batch_predict_advanced(self, file_name, output_dir='data_output/advanced_agentic_preds/', output_file='preds.jsonl'):\n",
    "        \n",
    "        data = self.JSON_read(file_name)\n",
    "        batch_results = []\n",
    "        \n",
    "        for i, record in enumerate(tqdm(data, desc=f\"Processing Test Data Predictions ({file_name})\")):\n",
    "            self.main(start_prompt=record)\n",
    "        \n",
    "            if self.batch_output:\n",
    "                result = {\n",
    "                    'predicted_price': self.batch_output['price'],\n",
    "                    'pred_id':i,\n",
    "                    'context':self.prediction_context\n",
    "                }\n",
    "                batch_results.append(result)\n",
    "        \n",
    "        # Saving results as JSONL\n",
    "        \n",
    "        output_dir = Path(output_dir) \n",
    "        start = Path(file_name).stem #before the .jsonl\n",
    "        end = Path(file_name).suffix\n",
    "        output_file_name = f'{start}_preds{end}'\n",
    "        output_path = output_dir / output_file_name\n",
    "        \n",
    "        with output_path.open('w', encoding='utf-8') as f:\n",
    "            for result in batch_results:\n",
    "                f.write(json.dumps(result) + '\\n')\n",
    "        \n",
    "        print(f'Saved {len(batch_results)} predictions at {str(output_path)}')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f8eee2db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions: 100%|██████████| 100/100 [56:44<00:00, 34.04s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/advanced_agentic_preds/X_test_part_0_preds.jsonl\n",
      "Predictions were completed in 56.74 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''start_time = time.time()\n",
    "\n",
    "agent_advanced = MultiAgent(agent_list=agent_list)\n",
    "chunk1_response = agent_advanced.batch_predict_advanced(file_name='X_test_part_0.jsonl')\n",
    "\n",
    "end_time = time.time()\n",
    "duration = end_time - start_time\n",
    "print(f'Predictions were completed in {(duration/60):.2f} minutes.')'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ed588b3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_1.jsonl): 100%|██████████| 100/100 [47:31<00:00, 28.52s/it] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/advanced_agentic_preds/X_test_part_1_preds.jsonl\n",
      "Predictions for X_test_part_1.jsonl were completed in 47.53 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_2.jsonl): 100%|██████████| 100/100 [43:29<00:00, 26.10s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/advanced_agentic_preds/X_test_part_2_preds.jsonl\n",
      "Predictions for X_test_part_2.jsonl were completed in 43.50 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_3.jsonl): 100%|██████████| 100/100 [44:36<00:00, 26.77s/it]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/advanced_agentic_preds/X_test_part_3_preds.jsonl\n",
      "Predictions for X_test_part_3.jsonl were completed in 44.61 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Processing Test Data Predictions (X_test_part_4.jsonl): 100%|██████████| 100/100 [48:53<00:00, 29.33s/it]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved 100 predictions at data_output/advanced_agentic_preds/X_test_part_4_preds.jsonl\n",
      "Predictions for X_test_part_4.jsonl were completed in 48.89 minutes.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "'''\n",
    "output_dir = 'data_output/advanced_agentic_preds'\n",
    "files_considered  = ['X_test_part_1.jsonl', \n",
    "                     'X_test_part_2.jsonl', \n",
    "                     'X_test_part_3.jsonl',\n",
    "                     'X_test_part_4.jsonl']\n",
    "\n",
    "for file in files_considered:\n",
    "    try:\n",
    "        start_time = time.time()\n",
    "\n",
    "        agent_advanced = MultiAgent(agent_list=agent_list)\n",
    "        chunk1_response = agent_advanced.batch_predict_advanced(file_name=file, output_dir='data_output/advanced_agentic_preds/')\n",
    "\n",
    "        end_time = time.time()\n",
    "        duration = end_time - start_time\n",
    "        \n",
    "        print(f'Predictions for {file} were completed in {(duration/60):.2f} minutes.')\n",
    "    except:\n",
    "        print('***')\n",
    "        print(f'Something went wrong with {file}')\n",
    "        print('***')\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "id": "8d3b7b2a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['X_test_part_0_preds.jsonl']\n",
      "['y_test_part_0.jsonl']\n",
      "Test RMSE: 311629.8876407717 \n",
      "Test MAE: 188080.9 \n",
      "Test MAPE: 28.96%\n"
     ]
    }
   ],
   "source": [
    "# Evaluation\n",
    "\n",
    "# Iterating over all available the chunks for the predicitons\n",
    "number_of_partitions_Advanced = 1 # Only 1 partition was predicted using openAI\n",
    "y_actuals_list = []\n",
    "y_preds_list = []\n",
    "\n",
    "for n in range(number_of_partitions_Advanced): \n",
    "    y_actuals_file = f'y_test_part_{str(n)}.jsonl'\n",
    "    y_preds_file = f'X_test_part_{str(n)}_preds.jsonl'\n",
    "    \n",
    "    y_actuals_list.append(y_actuals_file)\n",
    "    y_preds_list.append(y_preds_file)\n",
    "\n",
    "print(y_preds_list)\n",
    "print(y_actuals_list)\n",
    "\n",
    "\n",
    "rmse_AIA = []\n",
    "mae_AIA = []\n",
    "mape_AIA = []\n",
    "\n",
    "for actual,pred in zip(y_actuals_list, y_preds_list):\n",
    "    y_preds_fp = Path('data_output/advanced_agentic_preds/') / pred\n",
    "    y_actuals_fp = Path('data_output/prod_features/json_test_files/') / actual\n",
    "\n",
    "    y_preds_AI = pd.read_json(y_preds_fp, lines=True)\n",
    "    y_preds_AI = y_preds_AI['predicted_price'] # I only need the predicted price column.\n",
    "    y_actuals= pd.read_json(y_actuals_fp, lines=True, chunksize=None)\n",
    "\n",
    "    rmse_AIA = np.sqrt(mean_squared_error(y_actuals, y_preds_AI))\n",
    "    rmse_AI.append(test_rmse_AI)\n",
    "    test_mae_AI = mean_absolute_error(y_actuals,y_preds_AI)\n",
    "    mae_AIA.append(test_mae_AI)\n",
    "    test_mape_AI = round(mean_absolute_percentage_error(y_actuals,y_preds_AI)*100,2) \n",
    "    mape_AIA.append(test_mape_AI)\n",
    "\n",
    "print(f'Test RMSE: {test_rmse_AI} \\nTest MAE: {test_mae_AI} \\nTest MAPE: {test_mape_AI}%')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
